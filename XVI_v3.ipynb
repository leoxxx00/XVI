{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+JDAulXn5q42q/XIHnc58"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3p8oD_E32X9j","executionInfo":{"status":"ok","timestamp":1721071433826,"user_tz":-60,"elapsed":18478,"user":{"displayName":"LEON","userId":"05960493711730071836"}},"outputId":"50a3e5bc-eae6-4b0d-c9a8-6033627014dd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"6oqku97HrYSE","executionInfo":{"status":"error","timestamp":1721071438781,"user_tz":-60,"elapsed":504,"user":{"displayName":"LEON","userId":"05960493711730071836"}},"outputId":"6ad5fa61-839d-406e-bb6a-50accd0b3a93"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unterminated string literal (detected at line 235) (<ipython-input-3-f5937326123f>, line 235)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-f5937326123f>\"\u001b[0;36m, line \u001b[0;32m235\u001b[0m\n\u001b[0;31m    print(f\"Best Validation Accuracy: {best_val_acc} at epoch\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 235)\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n","from torchvision import transforms, models\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n","import seaborn as sns\n","from google.colab import drive\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import gradio as gr\n","from collections import Counter\n","import optuna\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Define paths\n","base_path = '/content/drive/MyDrive/XVI/Datas'\n","metadata_path = '/path/to/Data_Entry_2017.csv'  # Update this path accordingly\n","\n","# Define new classes from the ChestX-ray8 dataset\n","classes = [\n","    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule',\n","    'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema',\n","    'Fibrosis', 'Pleural_Thickening', 'Hernia'\n","]\n","\n","# Load metadata\n","metadata = pd.read_csv(metadata_path)\n","\n","# Create a mapping from image index to the label\n","label_mapping = {row['Image Index']: row['Finding Labels'].split('|') for idx, row in metadata.iterrows()}\n","\n","# Convert labels to numeric classes\n","all_labels = set()\n","for labels in label_mapping.values():\n","    all_labels.update(labels)\n","label_to_idx = {label: idx for idx, label in enumerate(sorted(all_labels))}\n","\n","# Create image paths and labels\n","image_paths = []\n","labels = []\n","\n","for image_name, diseases in label_mapping.items():\n","    image_path = os.path.join(base_path, image_name)\n","    if os.path.exists(image_path):\n","        image_paths.append(image_path)\n","        label_vector = [0] * len(label_to_idx)\n","        for disease in diseases:\n","            if disease in label_to_idx:\n","                label_vector[label_to_idx[disease]] = 1\n","        labels.append(label_vector)\n","\n","# Custom Dataset class\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx]).convert('RGB')\n","        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n","        if self.transform:\n","            image = self.transform(image=np.array(image))['image']\n","        return image, label\n","\n","# Define transforms with data augmentation using Albumentations\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","transform = A.Compose([\n","    A.Resize(150, 150),\n","    A.HorizontalFlip(),\n","    A.VerticalFlip(),\n","    A.RandomRotate90(),\n","    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    A.OneOf([\n","        A.GaussianBlur(),\n","        A.GaussNoise(),\n","        A.MotionBlur()\n","    ], p=0.3),\n","    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","    ToTensorV2()\n","])\n","\n","# Create the dataset\n","dataset = CustomImageDataset(image_paths, labels, transform=transform)\n","\n","# Split the data into training, devset, and test sets\n","partitions = [0.8, 0.1, 0.1]\n","\n","train_size = int(partitions[0] * len(dataset))\n","test_dev_size = len(dataset) - train_size\n","dev_size = int(partitions[1] * len(dataset))\n","test_size = test_dev_size - dev_size\n","\n","train_data, test_dev_data = random_split(dataset, [train_size, test_dev_size])\n","dev_data, test_data = random_split(test_dev_data, [dev_size, test_size])\n","\n","# Calculate class weights\n","class_counts = Counter([label.argmax() for label in labels])\n","total_samples = sum(class_counts.values())\n","class_weights = {cls: total_samples / class_counts.get(cls, 1) for cls in range(len(classes))}\n","\n","# Debugging: Print class weights and class counts\n","print(f\"class_counts: {class_counts}\")\n","print(f\"class_weights: {class_weights}\")\n","\n","sample_weights = [class_weights[label.argmax()] for label in labels]\n","\n","# Create samplers\n","train_sampler = WeightedRandomSampler(weights=sample_weights[:train_size], num_samples=train_size, replacement=True)\n","train_loader = DataLoader(train_data, batch_size=32, sampler=train_sampler)\n","dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n","\n","# Using a pre-trained ResNet50 model with additional layers and batch normalization\n","class FineTunedResNet(nn.Module):\n","    def __init__(self):\n","        super(FineTunedResNet, self).__init__()\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)  # Updated for weights\n","\n","        # Replace the fully connected layer with more layers and batch normalization\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 1024),  # First additional layer\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 512),  # Second additional layer\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, 256),  # Third additional layer\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, len(classes))  # Output layer, Softmax will be applied in the loss function\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Get the best hyperparameters from Optuna study\n","best_params = {'lr': 0.00028877715384763937, 'weight_decay': 0.0010977517310350644}\n","print('Best hyperparameters:', best_params)\n","\n","# Train the final model with the best hyperparameters\n","model = FineTunedResNet().to(device)\n","class_weights_tensor = torch.tensor([class_weights[i] for i in range(len(classes))], device=device)\n","criterion = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n","optimizer = optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Function to train and validate the model with early stopping\n","def train_validate_early_stopping(model, train_loader, dev_loader, criterion, optimizer, scheduler, num_epochs, save_path, patience):\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","    best_val_acc = 0.0\n","    best_epoch = 0\n","    patience_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            preds = (outputs > 0.5).float()\n","            correct += (preds == labels).sum().item()\n","            total += labels.numel()\n","\n","        train_losses.append(running_loss / len(train_loader))\n","        train_accuracies.append(correct / total)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for images, labels in dev_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                preds = (outputs > 0.5).float()\n","                correct += (preds == labels).sum().item()\n","                total += labels.numel()\n","\n","        val_losses.append(val_loss / len(dev_loader))\n","        val_accuracies.append(correct / total)\n","\n","        # Save the best model based on validation accuracy\n","        if val_accuracies[-1] > best_val_acc:\n","            best_val_acc = val_accuracies[-1]\n","            best_epoch = epoch\n","            patience_counter = 0\n","            torch.save(model.state_dict(), save_path)\n","        else:\n","            patience_counter += 1\n","\n","        # Early stopping\n","        if patience_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(dev_loader)}, Train Acc: {train_accuracies[-1]}, Val Acc: {val_accuracies[-1]}')\n","\n","    print(f\"Best Validation Accuracy: {best_val_acc} at epoch\n"]}]}